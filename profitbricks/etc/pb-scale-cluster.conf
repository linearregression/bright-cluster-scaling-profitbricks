########################################################
# Configuration file example for cm-scale-cluster.     #
# See man `cm-scale-cluster` or Bright Cluster Manager #
# administrator manual for details.                    #
########################################################

# Queues description: one line per queue. QUEUE takes the name of
# a job queue and extracts jobs information from that queue.
# If the queue name is asterisk (*), then all jobs will be dealt with.
# Each queue can take the following parameters set as key=value pairs:
#
# NODEGROUP        - group of nodes that will be started/stopped/terminated
#                    according jobs in specified queues (mandatory if TEMPLATE
#                    is not defined);
# NEVER_TERMINATE  - number of nodes that are never terminated (ie stay UP)
#                    even if no jobs need them. Default: 0
# JOBS_PER_NODE    - how many jobs can use a node at same time. Default: 1
# TEMPLATE         - node which will be used to clone new nodes
#                    (mandatory if NODEGROUP is not defined)
# EXTRA_NODES      - list of comma-separated extra node names.
# WHOLE_TIME       - a compute node running time (in minutes) before it is stopped
#                    if no jobs require it. If this parameter is set to 0 then when
#                    a node is not used (no jobs can be started or running there)
#                    then it will be stopped or terminated immediately. When 
#                    WHOLE_TIME is greater then 0 then the node will be stopped
#                    only if the following condition is satisfied:
#                    (T1-T2)%WHOLE_TIME + RUN_INTERVAL > WHOLE_TIME - STOPPING_ALLOWANCE_PERIOD,
#                    where T1 is a time when the condition is validated, T2 is a time
#                    when the node was started, and STOPPING_ALLOWANCE_PERIOD is a parameter
#                    set in this configuration file. Default: 0
# STOPPING_ALLOWANCE_PERIOD
#                  - a time just before the end of the WHOLE_TIME period prior to which all
#                    power off (or terminate) operations must be started. This offset
#                    should guarantee that if cm-scale-cluster initiates power off or
#                    terminate, there should be enough time before the end of
#                    the WHOLE_TIME period to complete the power operation. Default: 0
# ASSIGN_CATEGORY  - a node category name that should be assigned to the managed nodes.

#QUEUE=queue1 TEMPLATE=cnode001 EXTRA_NODES=us-west-1-director
#QUEUE=queue2 NODEGROUP=nodegroup1 EXTRA_NODES=provisioning-server
#QUEUE=queue3 NODEGROUP=nodegroup2 TEMPLATE=cnode001 WHOLE_TIME=60 STOPPING_ALLOWANCE_PERIOD=10
#QUEUE=queue4 NODEGROUP=pool ASSIGN_CATEGORY=cat1
#QUEUE=queue5 NODEGROUP=pool ASSIGN_CATEGORY=cat2
QUEUE=defq NODEGROUP=pbnodes

# RESOURCE description: one line per resource requested by a job resource name.
# RESOURCE extracts job information from all queues for a resource specified
# in the job request.
# A RESOURCE line supports the same options that a QUEUE line supports. In
# addition, each resource (key) that is to be used must be specified, and
# assigned a value.
# For instance, if the user requests gpu=tesla, then the job will be dealt with
# if the RESOURCE line has RESOURCE=gpu and VALUE=tesla.
# If a wildcard (*) is used as the VALUE, then all jobs that requested the resource
# will be dealt with, whatever the value assigned to the resource.
# NOTE: RESOURCE parameter support is experimental.

#RESOURCE=gpu VALUE=* NODEGROUP=gpunodes

# Template node description: one line per template. TEMPLATE takes the name of
# a node. TEMPLATE can take the following parameters set as key=value pairs:
#
# START              - is template node also allowed to start automatically. Default: NO
# STOP               - is template node also allowed to stop automatically. Default: NO
# NODES              - range of new nodes that can be cloned from the template node
# REMOVE             - should new node be removed from Bright Cluster Manager
#                      configuration upon the node termination. If node is not going to
#                      be terminated (just stopped), then it will never be removed
#                      Default: NO
# LEAVE_FAILED_NODES - are failed nodes should not be touched in order to allow
#                      administrator to investigate why they were failed. Default: NO
# INTERFACE          - network interface. Its IP address will be increased by 1 (IPv4)
#                      when the node is cloned. If the value is IGNORE then no IP address
#                      will be incrimented. Default: tun0

#TEMPLATE=cnode001 NODES=cnode[03..32] INTERFACE=IGNORE

# Extra node description: one line per extra node. EXTRA_NODE takes the name of
# an extra node that is always needed. Typically for cloud nodes it is the
# cloud director, for regular nodes it is a provisioning node. Each EXTRA_NODE
# can have the following parameters set as key=value pairs:
#
# IDLE_TIMEOUT - time, in seconds, that extra nodes can remain unused. After this
#                time they are stopped. Default: 3600
# START        - automatically start extra node before the first compute node
#                is started. Default: YES
# STOP         - automatically stop extra node after the last compute node
#                stops. Default: YES

#EXTRA_NODE=us-west-1-director

# Run interval description: interval (in minutes) between cm-scale-cluster
# executions. This number is required when WHOLE_TIME is not 0. The interval
# is then used to calculate whether or not node can be stopped, or if it can 
# be stopped in the next execution of cm-scale-cluster, within current
# WHOLE_TIME period. Default: 5

RUN_INTERVAL = 5

# Print debug messages in log file.
DEBUG = YES

# Current workload manager (slurm,uge,sge,pbspro,torque,lsf,openlava).
WORKLOAD_MANAGER = slurm

# Path to script where power management policies are implemented
#POLICY_MODULE = /cm/local/apps/cm-scale-cluster/lib/default-policy.py

# Path to script where nodes power operatios are implemented
#ACTION_MODULE = /cm/local/apps/cm-scale-cluster/lib/default-action.py

# Maximum number of parallel requests to CMDaemon
#PARALLEL_REQUESTS = 10

# CMDaemon request timeout
#CMD_TIMEOUT = 5

# Path to cm-scale-cluster messages file
LOG_FILE  = /var/log/cm-scale-cluster.log

# Path to lock file
LOCK_FILE = /var/lock/subsys/cm-scale-cluster

# Directory where temporary files will be created
SPOOL_DIR = /var/spool/cmd

# Use slots number retrieved from workload manager dynamicaly (LSF only)
# AUTO_SLOTS = NO

